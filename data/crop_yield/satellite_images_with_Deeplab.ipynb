{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def read(file):\n",
    "    with rasterio.open(file) as src:\n",
    "        return src.read(), src.profile\n",
    "\n",
    "def get_dates(path, n=None):\n",
    "    files = os.listdir(path)\n",
    "    dates = list()\n",
    "    for f in files:\n",
    "        f = f.split(\"_\")[0]\n",
    "        if len(f) == 8:  # YYYYMMDD format\n",
    "            dates.append(f)\n",
    "    dates = set(dates)\n",
    "    if n is not None:\n",
    "        dates = random.sample(dates, n)\n",
    "    dates = list(dates)\n",
    "    dates.sort()\n",
    "    return dates\n",
    "\n",
    "class SentinelDataset(Dataset):\n",
    "    def __init__(self, root_dirs, seqlength=30, tileids_paths=None, preprocess_dirs=None, save_preprocessed=False):\n",
    "        self.root_dirs = [r.rstrip(\"/\") for r in root_dirs]\n",
    "        self.data_dirs = ['data2016']\n",
    "        self.seqlength = seqlength\n",
    "        self.samples = list()\n",
    "        self.ndates = list()\n",
    "        self.classids, self.classes = self.read_classes()\n",
    "        self.preprocess_dirs = preprocess_dirs\n",
    "        self.save_preprocessed = save_preprocessed\n",
    "        self.tileids_paths = tileids_paths\n",
    "        self.prepare_dataset()\n",
    "\n",
    "    def read_classes(self):\n",
    "        classes_path = 'data/sentinel2/lombardia-classes/classes25pc.txt'\n",
    "        with open(classes_path, 'r') as f:\n",
    "            classes = f.readlines()\n",
    "        ids, names = [], []\n",
    "        for row in classes:\n",
    "            row = row.replace(\"\\n\", \"\")\n",
    "            if '|' in row:\n",
    "                cls_info = row.split('|')\n",
    "                id_info = [int(x) for x in cls_info[0].split(',')]\n",
    "                ids.append(id_info)\n",
    "                names.append(cls_info[1])\n",
    "        return ids, names\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        for root_dir, tileids_path, preprocess_dir in zip(self.root_dirs, self.tileids_paths, self.preprocess_dirs):\n",
    "            print(\"Reading dataset info:\", root_dir)\n",
    "            dirs = []\n",
    "            if tileids_path is None:\n",
    "                for d in self.data_dirs:\n",
    "                    dirs_name = os.listdir(os.path.join(root_dir, d))\n",
    "                    dirs_path = [os.path.join(root_dir, d, f) for f in dirs_name]\n",
    "                    dirs.extend(dirs_path)\n",
    "            else:\n",
    "                try:\n",
    "                    with open(tileids_path, 'r') as f:\n",
    "                        files = [el.replace(\"\\n\", \"\") for el in f.readlines()]\n",
    "                    for d in self.data_dirs:\n",
    "                        dirs_path = [os.path.join(root_dir, d, f) for f in files]\n",
    "                        dirs.extend(dirs_path)\n",
    "                except PermissionError as e:\n",
    "                    print(f\"Permission error: {e}\")\n",
    "                    continue\n",
    "                except FileNotFoundError as e:\n",
    "                    print(f\"File not found: {e}\")\n",
    "                    continue\n",
    "            stats = dict(rejected_nopath=0, rejected_length=0, total_samples=0)\n",
    "            for path in tqdm(dirs):\n",
    "                if not os.path.exists(path) or not os.path.exists(os.path.join(path, \"y.tif\")):\n",
    "                    stats[\"rejected_nopath\"] += 1\n",
    "                    continue\n",
    "                ndates = len(get_dates(path))\n",
    "                if ndates < self.seqlength:\n",
    "                    stats[\"rejected_length\"] += 1\n",
    "                    continue\n",
    "                stats[\"total_samples\"] += 1\n",
    "                self.samples.append((path, preprocess_dir))\n",
    "                self.ndates.append(ndates)\n",
    "            self.print_stats(stats)\n",
    "            if self.save_preprocessed:\n",
    "                self.save_preprocessed_data()\n",
    "\n",
    "    def save_preprocessed_data(self):\n",
    "        for sample, preprocess_dir in self.samples:\n",
    "            label, profile = read(os.path.join(sample, \"y.tif\"))\n",
    "            dates = get_dates(sample, n=self.seqlength)\n",
    "            data = {\n",
    "                'label': label,\n",
    "                'profile': profile,\n",
    "                'dates': dates\n",
    "            }\n",
    "            save_path = os.path.join(preprocess_dir, os.path.basename(sample) + \".pkl\")\n",
    "            os.makedirs(preprocess_dir, exist_ok=True)\n",
    "            try:\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                print(f\"Saved preprocessed data to {save_path}\")\n",
    "            except PermissionError as e:\n",
    "                print(f\"Permission error: {e}\")\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"File not found: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_path, preprocess_dir = self.samples[idx]\n",
    "        label, profile = read(os.path.join(sample_path, \"y.tif\"))\n",
    "        dates = get_dates(sample_path, n=self.seqlength)\n",
    "        data_10m, data_20m, data_60m = [], [], []\n",
    "        for date in dates:\n",
    "            data_10m.append(read(os.path.join(sample_path, date + \"_10m.tif\"))[0])\n",
    "            data_20m.append(read(os.path.join(sample_path, date + \"_20m.tif\"))[0])\n",
    "            data_60m.append(read(os.path.join(sample_path, date + \"_60m.tif\"))[0])\n",
    "        data_10m = np.array(data_10m) * 1e-4\n",
    "        data_20m = np.array(data_20m) * 1e-4\n",
    "        data_60m = np.array(data_60m) * 1e-4\n",
    "        data_20m = torch.from_numpy(data_20m)\n",
    "        data_60m = torch.from_numpy(data_60m)\n",
    "        data_20m = torch.nn.functional.interpolate(data_20m, size=data_10m.shape[2:4])\n",
    "        data_60m = torch.nn.functional.interpolate(data_60m, size=data_10m.shape[2:4])\n",
    "        data = torch.cat((torch.from_numpy(data_10m), data_20m, data_60m), 1)\n",
    "        data = data.permute(1, 0, 2, 3).float()\n",
    "        label = torch.from_numpy(label).long()\n",
    "        return data, label\n",
    "\n",
    "    def print_stats(self, stats):\n",
    "        print_lst = list()\n",
    "        for k, v in zip(stats.keys(), stats.values()):\n",
    "            print_lst.append(\"{}:{}\".format(k, v))\n",
    "        print('\\n', \", \".join(print_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and parameters\n",
    "root_paths = ['data/sentinel2/lombardia', 'data/sentinel2/lombardia2']\n",
    "preprocess_dirs = ['data/sentinel2/lombardia_preprocessed', 'data/sentinel2/lombardia2_preprocessed']\n",
    "tileids_paths = ['data/sentinel2/lombardia/tileids/train_fold0.tileids', 'data/sentinel2/lombardia2/tileids/train_fold0.tileids']\n",
    "\n",
    "# Initialize and prepare dataset\n",
    "dataset = SentinelDataset(root_paths, seqlength=30, tileids_paths=tileids_paths, preprocess_dirs=preprocess_dirs, save_preprocessed=True)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.atrous_block1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.atrous_block6 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=6, dilation=6, bias=False)\n",
    "        self.atrous_block12 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=12, dilation=12, bias=False)\n",
    "        self.atrous_block18 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=18, dilation=18, bias=False)\n",
    "        self.conv_1x1_output = nn.Conv2d(out_channels * 4, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.atrous_block1(x)\n",
    "        x2 = self.atrous_block6(x)\n",
    "        x3 = self.atrous_block12(x)\n",
    "        x4 = self.atrous_block18(x)\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = self.conv_1x1_output(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.relu(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "class DeepLabV3(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3, pretrained=True):\n",
    "        super(DeepLabV3, self).__init__()\n",
    "        self.backbone = resnet50(pretrained=pretrained)\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        self.aspp = ASPP(2048, 256)\n",
    "        self.conv1 = nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.aspp(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return F.interpolate(x, scale_factor=8, mode='bilinear', align_corners=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 22\n",
    "model = DeepLabV3(num_classes=num_classes, in_channels=3, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assuming 'dataset' is already created from Step 1\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "val_split = 0.2\n",
    "train_size = int((1 - val_split) * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
    "    step_size = trial.suggest_int('step_size', 5, 20)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "\n",
    "    # Create DataLoader with suggested batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize model, criterion, optimizer, and scheduler\n",
    "    model = DeepLabV3(num_classes=num_classes, in_channels=3, pretrained=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Move model to device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop for one epoch\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation loop for one epoch\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "print(\"Best validation loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters from Optuna\n",
    "best_params = study.best_params\n",
    "\n",
    "# Create DataLoader with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "# Initialize the model, criterion, optimizer, and scheduler with best hyperparameters\n",
    "model = DeepLabV3(num_classes=num_classes, in_channels=3, pretrained=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "scheduler = StepLR(optimizer, step_size=best_params['step_size'], gamma=best_params['gamma'])\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_indices, val_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Create DataLoader with the best batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, criterion, optimizer, and scheduler with best hyperparameters\n",
    "model = DeepLabV3(num_classes=num_classes, in_channels=3, pretrained=False)\n",
    "model.load_state_dict(torch.load('/path/to/resnet50-0676ba61.pth'))  # Load pretrained weights\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "scheduler = StepLR(optimizer, step_size=best_params['step_size'], gamma=best_params['gamma'])\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(labels, preds):\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    \n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "val_labels, val_preds = evaluate_model(model, val_loader, device)\n",
    "\n",
    "# Compute metrics\n",
    "accuracy, precision, recall, f1, cm = compute_metrics(val_labels, val_preds)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = dataset.classes\n",
    "plot_confusion_matrix(cm, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
